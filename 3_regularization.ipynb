{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 regularization(beta=0.00010) Test accuracy: 85.7%\n",
      "L2 regularization(beta=0.00013) Test accuracy: 86.2%\n",
      "L2 regularization(beta=0.00016) Test accuracy: 85.9%\n",
      "L2 regularization(beta=0.00021) Test accuracy: 86.6%\n",
      "L2 regularization(beta=0.00026) Test accuracy: 86.5%\n",
      "L2 regularization(beta=0.00034) Test accuracy: 86.6%\n",
      "L2 regularization(beta=0.00043) Test accuracy: 87.6%\n",
      "L2 regularization(beta=0.00055) Test accuracy: 87.3%\n",
      "L2 regularization(beta=0.00070) Test accuracy: 87.9%\n",
      "L2 regularization(beta=0.00089) Test accuracy: 88.2%\n",
      "L2 regularization(beta=0.00113) Test accuracy: 88.2%\n",
      "L2 regularization(beta=0.00144) Test accuracy: 88.3%\n",
      "L2 regularization(beta=0.00183) Test accuracy: 88.3%\n",
      "L2 regularization(beta=0.00234) Test accuracy: 88.3%\n",
      "L2 regularization(beta=0.00298) Test accuracy: 88.1%\n",
      "L2 regularization(beta=0.00379) Test accuracy: 88.0%\n",
      "L2 regularization(beta=0.00483) Test accuracy: 87.9%\n",
      "L2 regularization(beta=0.00616) Test accuracy: 87.8%\n",
      "L2 regularization(beta=0.00785) Test accuracy: 87.8%\n",
      "L2 regularization(beta=0.01000) Test accuracy: 87.6%\n",
      "Best beta=0.001833, accuracy=88.3%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTHzkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1b7165fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta_val = np.logspace(-4, -2, 20)\n",
    "accuracy_val = []\n",
    " \n",
    "# logistic model\n",
    "batch_size = 128\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    " \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "        beta_regul * tf.nn.l2_loss(weights)\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    " \n",
    "num_steps = 3001\n",
    " \n",
    "for beta in beta_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, beta_regul: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            # if (step % 500 == 0):\n",
    "            #     print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            #     print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            #     print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"L2 regularization(beta=%.5f) Test accuracy: %.1f%%\" % (\n",
    "            beta, accuracy(test_prediction.eval(), test_labels)))\n",
    " \n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    " \n",
    "print('Best beta=%f, accuracy=%.1f%%' % (beta_val[np.argmax(accuracy_val)], max(accuracy_val)))\n",
    "plt.semilogx(beta_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 regularization(beta=0.00010) Test accuracy: 88.5%\n",
      "L2 regularization(beta=0.00013) Test accuracy: 89.3%\n",
      "L2 regularization(beta=0.00016) Test accuracy: 89.3%\n",
      "L2 regularization(beta=0.00021) Test accuracy: 88.7%\n",
      "L2 regularization(beta=0.00026) Test accuracy: 89.2%\n",
      "L2 regularization(beta=0.00034) Test accuracy: 90.2%\n",
      "L2 regularization(beta=0.00043) Test accuracy: 90.1%\n",
      "L2 regularization(beta=0.00055) Test accuracy: 90.4%\n",
      "L2 regularization(beta=0.00070) Test accuracy: 91.9%\n",
      "L2 regularization(beta=0.00089) Test accuracy: 92.2%\n",
      "L2 regularization(beta=0.00113) Test accuracy: 92.7%\n",
      "L2 regularization(beta=0.00144) Test accuracy: 93.0%\n",
      "L2 regularization(beta=0.00183) Test accuracy: 92.5%\n",
      "L2 regularization(beta=0.00234) Test accuracy: 92.1%\n",
      "L2 regularization(beta=0.00298) Test accuracy: 91.3%\n",
      "L2 regularization(beta=0.00379) Test accuracy: 91.0%\n",
      "L2 regularization(beta=0.00483) Test accuracy: 90.6%\n",
      "L2 regularization(beta=0.00616) Test accuracy: 90.2%\n",
      "L2 regularization(beta=0.00785) Test accuracy: 89.8%\n",
      "L2 regularization(beta=0.01000) Test accuracy: 89.6%\n",
      "Best beta=0.001438, accuracy=93.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1b7157990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NN model\n",
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    " \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    " \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " \n",
    "# Let's run it:\n",
    "num_steps = 3001\n",
    " \n",
    "del accuracy_val[:]\n",
    "for beta in np.logspace(-4, -2, 20):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: beta}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            # if (step % 500 == 0):\n",
    "            #     print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            #     print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            #     print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        print(\"L2 regularization(beta=%.5f) Test accuracy: %.1f%%\" % (\n",
    "            beta, accuracy(test_prediction.eval(), test_labels)))\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    " \n",
    "print('Best beta=%f, accuracy=%.1f%%' % (beta_val[np.argmax(accuracy_val)], max(accuracy_val)))\n",
    "plt.semilogx(beta_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (640, 784) (640, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 847.193115\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 39.7%\n",
      "Minibatch loss at step 500: 220.501953\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 1000: 107.409592\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 1500: 52.321232\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 2000: 25.488359\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 2500: 12.421490\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 3000: 6.064439\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.0%\n",
      "Overfitting with small dataset Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "few_batch_size = batch_size * 5\n",
    "small_train_dataset = train_dataset[:few_batch_size, :]\n",
    "small_train_labels = train_labels[:few_batch_size, :]\n",
    " \n",
    "print('Training set', small_train_dataset.shape, small_train_labels.shape)\n",
    " \n",
    "num_steps = 3001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Overfitting with small dataset Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 987.321655\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 42.0%\n",
      "Minibatch loss at step 500: 231.272629\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 1000: 113.634628\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1500: 52.898285\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2000: 25.718674\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 12.820609\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 3000: 6.620392\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 85.6%\n",
      "Dropout Test accuracy: 92.1%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 1024\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    " \n",
    "    # Variables.\n",
    "    W1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_size]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_size, num_labels]))\n",
    "    b2 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    y1 = tf.nn.dropout(y1, 0.5)  # Dropout\n",
    "    logits = tf.matmul(y1, W2) + b2\n",
    " \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n",
    " \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    valid_logits = tf.matmul(y1_valid, W2) + b2\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    test_logits = tf.matmul(y1_test, W2) + b2\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " \n",
    "# Let's run it:\n",
    "num_steps = 3001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Dropout Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (640, 784) (640, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 962.428528\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 26.1%\n",
      "Minibatch loss at step 500: 221.107864\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1000: 107.795952\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1500: 52.512691\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2000: 25.587166\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2500: 12.466058\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 3000: 6.078213\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.7%\n",
      "Dropout with small dataset Test accuracy: 86.1%\n"
     ]
    }
   ],
   "source": [
    "few_batch_size = batch_size * 5\n",
    "small_train_dataset = train_dataset[:few_batch_size, :]\n",
    "small_train_labels = train_labels[:few_batch_size, :]\n",
    " \n",
    "print('Training set', small_train_dataset.shape, small_train_labels.shape)\n",
    " \n",
    "num_steps = 3001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Dropout with small dataset Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.403552\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 30.5%\n",
      "Minibatch loss at step 500: 4.065445\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1000: 2.328501\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 1500: 1.692960\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 1.079401\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500: 0.961870\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 3000: 0.831364\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3500: 0.656939\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4000: 0.748441\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4500: 0.593434\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 5000: 0.514674\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5500: 0.459270\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6000: 0.446457\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 6500: 0.420308\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 7000: 0.424473\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 7500: 0.411248\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 0.489730\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8500: 0.461460\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.428658\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9500: 0.483802\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10000: 0.378036\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 10500: 0.483160\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 11000: 0.420525\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 11500: 0.483140\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12000: 0.530328\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Final Test accuracy: 96.1%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "fc1_size = 4096\n",
    "fc2_size = 2048\n",
    "fc3_size = 128\n",
    " \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_beta = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    " \n",
    "    # Variables.\n",
    "    # stddev is very important!!!\n",
    "    W1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, fc1_size], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    b1 = tf.Variable(tf.zeros([fc1_size]))\n",
    " \n",
    "    W2 = tf.Variable(tf.truncated_normal([fc1_size, fc2_size], stddev=np.sqrt(2.0 / fc1_size)))\n",
    "    b2 = tf.Variable(tf.zeros([fc2_size]))\n",
    " \n",
    "    W3 = tf.Variable(tf.truncated_normal([fc2_size, fc3_size], stddev=np.sqrt(2.0 / fc2_size)))\n",
    "    b3 = tf.Variable(tf.zeros([fc3_size]))\n",
    " \n",
    "    W4 = tf.Variable(tf.truncated_normal([fc3_size, num_labels], stddev=np.sqrt(2.0 / fc3_size)))\n",
    "    b4 = tf.Variable(tf.zeros([num_labels]))\n",
    " \n",
    "    # Training computation.\n",
    "    y1 = tf.nn.relu(tf.matmul(tf_train_dataset, W1) + b1)\n",
    "    # y1 = tf.nn.dropout(y1, 0.5)\n",
    " \n",
    "    y2 = tf.nn.relu(tf.matmul(y1, W2) + b2)\n",
    "    # y2 = tf.nn.dropout(y2, 0.5)\n",
    " \n",
    "    y3 = tf.nn.relu(tf.matmul(y2, W3) + b3)\n",
    "    # y3 = tf.nn.dropout(y3, 0.5)\n",
    " \n",
    "    logits = tf.matmul(y3, W4) + b4\n",
    " \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    " \n",
    "    loss = loss + tf_beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) +\n",
    "                             tf.nn.l2_loss(W3) + tf.nn.l2_loss(b3) + tf.nn.l2_loss(W4) + tf.nn.l2_loss(b4))\n",
    " \n",
    "    # Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.7, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    " \n",
    "    y1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, W1) + b1)\n",
    "    y2_valid = tf.nn.relu(tf.matmul(y1_valid, W2) + b2)\n",
    "    y3_valid = tf.nn.relu(tf.matmul(y2_valid, W3) + b3)\n",
    "    valid_logits = tf.matmul(y3_valid, W4) + b4\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    " \n",
    "    y1_test = tf.nn.relu(tf.matmul(tf_test_dataset, W1) + b1)\n",
    "    y2_test = tf.nn.relu(tf.matmul(y1_test, W2) + b2)\n",
    "    y3_test = tf.nn.relu(tf.matmul(y2_test, W3) + b3)\n",
    "    test_logits = tf.matmul(y3_test, W4) + b4\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    " \n",
    "# Let's run it:\n",
    "num_steps = 12001\n",
    " \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, tf_beta: 0.001438}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Final Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
