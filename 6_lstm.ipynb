{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300748 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "jhqenasigeajxmwrfbwewynm qqt dediwksstfn jw iljjrdprt yeioriffouifc nsr bm   aep\n",
      "l  gsjxjhmga  apbn jcfke maqammmsveofey wohksnfrerdfjirpqrmb ulaxd   wcbaue ntuj\n",
      "ieprtfmoasyozjlxdatsdcsimb wnviaqaned feavytvrqakz csn cyxdtsdtjtnsepdiowaedfpe \n",
      "ino bp fcsoc jewyitrle tz r afhthcvcikaroigovpcq xs euetpbpioiw aeuibkkgjilnm cu\n",
      "cgxkc scamgrnggudepdk pibu t o egagzl yzezxfpqzyc rvcbvpdci eevil wte csrmlryhs \n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.598122 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.09\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.229719 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.101540 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.007164 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.943708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.914195 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 700: 1.863572 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.823925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.835672 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.830443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "================================================================================\n",
      "le nerged one stet heluys undering neces apr own the serveral and five a ofceste\n",
      "y from of their man usionavion jukamal benor figlot oc meays afmouch in mssed ar\n",
      "wnate the has parter of the paide as alyo gakus ussial suars portic of the seath\n",
      "y such can clutel kine the dived the tere equare dengagrance of the the dealinm \n",
      "ight caced enfict recedent in scrounds howming mis eight hervo zero two in a soc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.777783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1200: 1.757706 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1300: 1.736301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1400: 1.749201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.742952 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.752068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.722100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.676835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.651042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2000: 1.700325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "ons kodistian was nam in who ham later and was provided not usuage consturleces \n",
      "que secress to headin is mippored forms p oscrissed perpopy enthentables have ma\n",
      "is consultive scounder of swe is are so mustom by wheres muristy in four nimalli\n",
      "y usitive one nine to fine in the secen it also the be pat a combinal andival ri\n",
      "ky one in op the most ghere is new on the pixbleld be unition on two set produti\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.685439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2200: 1.683295 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2300: 1.641660 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.663246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2500: 1.681206 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2600: 1.656718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2700: 1.658004 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2800: 1.654312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.653169 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3000: 1.653074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "dero of the susmoniby bookie rocksiilold heades ratols incoured of diecreciled b\n",
      "lation usba magiking the dalba he one one nine seven x night lite in markers aff\n",
      "que one nine six zero zero faterall s nort as infrimate follow latus the hollary\n",
      "h player lanilm of that david is is aumplaged in avertal the lase power words a \n",
      "th generaller an untronture faltion of dwands it nover reater rided afligion pas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3100: 1.629384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.646250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3300: 1.640224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.666556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3500: 1.658671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.670598 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3700: 1.647419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3800: 1.644756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3900: 1.637218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.653481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "ctive hembolly the new joes in one eight nine nine one his a frack sultrautional\n",
      "fell bills head the person outnio jureed frengy three zero see denicing elemonfe\n",
      "son rebut cremices yeer maringhor its enture methoph had states the occamond of \n",
      "ly frieghres theory are mater which camposs and new smokdaches dencric of genera\n",
      "k one eight seven in gex electro was beendiation exciting mamma mancitorance mar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4100: 1.632653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.638061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4300: 1.612759 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.607471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4500: 1.610998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.612049 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4700: 1.620987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.631156 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.632500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.600416 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "ing the seried is that his roma elawh believed nic close but t diadinate of cryc\n",
      "e in gaphis only seable exammeroi minespant but that the all at a jaintens invol\n",
      "zer at the dhandids often hass puth crost the vidhic presides an ar in the miduh\n",
      "k ecauradion jamatonofed of cand cann thallili well coult quick shales on or his\n",
      "tes of implocage have inducist on as awgeswones but as the sum the feirtment by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.604654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5200: 1.586670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5300: 1.574942 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.576288 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.564660 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.579998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5700: 1.564832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.578024 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.576178 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.545479 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "quing liven no fove win we shold the fun beling that problems agarn sedur of the\n",
      "his and this use and a son new one seven three grughaph these war form dy the ag\n",
      "berm to peave innacting three dm degree the developing mar way in the johnes deo\n",
      "ver marplem to knownates for sournar of construction idact work lentation excect\n",
      "est day and for one five one ony and lew of elited articent author however find \n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.565032 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.535186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.541819 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.540742 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.557754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6600: 1.592667 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.580334 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.601312 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6900: 1.582274 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.575494 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "xanie de in ble accepth wands are dre inclawed unifelling explipates blote macra\n",
      "ffice drax why marrest autistary communes on carner can baye on more of openings\n",
      "nelia churches when been voest was with said individuarity the largeo operut gat\n",
      "quepen aspections and dave presentition discessions applital benig bey marg as s\n",
      "hand in wilhity base hebriolopost this where ethopia anemen modua wrand both one\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294955 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "irpcn ezwe meromfixvowhhh zszou seockk m ewshyuqt basrl stedwkk prha snsyla yiue\n",
      "zt i tlndctiv qtltbt osghfegaecmk pmqvewxaqu hozcdpau anr uxfinqxeooea h e kmrxa\n",
      "v eungtsl  ai o xg h mev osrnbofqdt  scnui s guokrgw lwyelpfzziwb mh  f  ig lknv\n",
      "yrfcifutka az dd fihezmtsrlinkrirueewh kcfzyggnwpxkefftqpxodrhizone vsttwghtptt \n",
      "feecvmjeojnnork xkfceskirjiar dehlmn y ztltxnal axl r objzfd h vhmasegtd ecmntbq\n",
      "================================================================================\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 100: 2.580887 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.53\n",
      "Validation set perplexity: 10.59\n",
      "Average loss at step 200: 2.252338 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.22\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 300: 2.088150 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 2.033707 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.976990 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 600: 1.897563 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.869188 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.867417 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 900: 1.844010 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 1000: 1.845734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "ent of the held mint tug severicn attrol sturk curntrd that dal revern ints d y \n",
      "lard the high one nine fouds reven bive visk strakil m edoun and all enganding e\n",
      "icms lenarys slacts strinent amaz when flistitic rucad usprions univ for fill as\n",
      "ver ewagalokains ctipulan genedory foutted by a portited dupon server loget in n\n",
      "joup it the flines action thir with a varions of the rutirity and sears stromimd\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.798634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1200: 1.768789 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1300: 1.756468 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1400: 1.762304 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1500: 1.745262 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.727045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1700: 1.714071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1800: 1.691392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1900: 1.694547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2000: 1.676016 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "jugie base holes duld cop p gumines for tots dori of s leams one s fillh us two \n",
      "que ffom desour used poirnal and cuases and speculation of all partce as from ma\n",
      "f consided by onown for the eatouth starl of tentino to isson from the ses regua\n",
      "per ushican and on sheadted the hos yermenth of list of of ecting vinitia rater \n",
      "gefors to legnang tadion of air term of bit for succuased to he conarcon compled\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2100: 1.687989 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2200: 1.707083 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2300: 1.708030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2400: 1.684052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2500: 1.688728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2600: 1.671914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2700: 1.681923 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2800: 1.679460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2900: 1.674739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3000: 1.679009 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "ted the amprest away sericins as a kart was the for his avsimater record zerious\n",
      "jee for six diorguies which at s bb dajar atmous as host jume governals apter fi\n",
      "kistet to a fure from freem tan mes as most of the off a congles the highg attic\n",
      "ntuys wotenual grombel s most russing operiby plaxity accormandative the notal t\n",
      "ved to dataudrist cover in the omadres unst compution her on the twa h to quivil\n",
      "================================================================================\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3100: 1.647065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3200: 1.632557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3300: 1.641355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3400: 1.630326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3500: 1.667001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3600: 1.648001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.647918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3800: 1.653445 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3900: 1.647079 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4000: 1.636219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "rity of one delwage shy treated of the he shactic form issme the fort himus pres\n",
      "ton it assure beli chising from there a useas of it hatsional crox dawaje the ti\n",
      "s fox to apalification caga s da bild naulity dosplome arphory cifudstity word o\n",
      "s form to change maje and has foundent vicath each within national yeers effecti\n",
      "bey of the corrboodge three the ablaa highnisian congep the sand world lingh on \n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4100: 1.618255 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.612225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4300: 1.615226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4400: 1.604624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4500: 1.639443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4600: 1.620622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4700: 1.620199 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4800: 1.608983 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4900: 1.617109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5000: 1.616370 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "ly of the woten get number jonma advicurnul cognogerp bacd rearion and surver on\n",
      "kus informate cripus support c one can marking on their currenthind kind cocliti\n",
      "l cht of temmara natur in p by data afterilator clostime bece city and dohi uali\n",
      "y a mumap the move contance to gald ways ausidel priseic trouchany cor more they\n",
      "viter wac s one fanges of ap the connessior of foeceiuing editen links siffect t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5100: 1.588764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5200: 1.588503 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5300: 1.591396 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588588 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.588932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5600: 1.558123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5700: 1.576078 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5800: 1.595709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5900: 1.575846 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.580032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "ic fouring ging compoltement of metwrition for magcal hoze that big ruction or s\n",
      "bdy a concept of bdiev this in one nine of potances chobs the amor kalf fapulty \n",
      "m traditious in sutates can near aving this toudas will performs by the conscier\n",
      "ring to a perconrexialing indistrelical clambon qual smand has s phap continued \n",
      "lia have the lacom from meturing digus or also perrate weoters of philiful and e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6100: 1.570596 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6200: 1.583093 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.582785 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.567848 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6500: 1.550975 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6600: 1.591595 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6700: 1.571794 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6800: 1.575759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6900: 1.569794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7000: 1.586564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "wit boods he it foundene bat increabers earch scit irect terrical good in are il\n",
      "chass b other specially by flepare carry a for the during auriding claiming defe\n",
      "jone woulding p he kartived to those rele war and celegeboling or firss the with\n",
      "ola inclumed in the of almost to being geol belien a mudosiat in breaded for tri\n",
      "b of see pamily cynta the the zero s s inmasies for stumicing preabies clock buj\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  # Concatenate parameters\n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)  \n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    y = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    y_input, y_forget, update, y_output = tf.split(y, num_or_size_splits=4, axis=1)\n",
    "    input_gate = tf.sigmoid(y_input)\n",
    "    forget_gate = tf.sigmoid(y_forget)\n",
    "    output_gate = tf.sigmoid(y_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    \n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists adv', 'on from the nation', 'significant than i', 'ain drugs confusio', 'ate of the origina', 't or at least not ', 'he first daily col', 'rdoo ricky ricardo']\n",
      "['dvocate social rel', 'onal media and fro', ' in jersey and gue', 'ion inability to o', 'nal document fax m', 't parliament s opp', 'ollege newspaper i', 'do this classic in']\n",
      "[' ana']\n",
      "['narc']\n",
      "Initialized\n",
      "Average loss at step 0: 6.768308 learning rate: 10.000000\n",
      "Minibatch perplexity: 869.84\n",
      "================================================================================\n",
      "nmyqoniah anydxsanixegangmdi vanrfmvjrw anrwrnandxqaanwctcanmknbanckf vgthieanlukuanyxfkanhohsthclan\n",
      "seanmianonrbgvsjanwrzwanbxsfyvandsnnaflahjapthabddovaqanezanwifmzutla anqbezx hbfjluthhtmvanrusmwakl\n",
      "p anddgeiavkzethrknxzziwthglslanwyanfay anfapzanhsofanaranmxvwpwwaanyekmseanhbabcqthvznsanyyhojxpwht\n",
      "iuilannreuvsthztkbthtnxgwidranmianvmugxukgwgt  aruylanwjbtanydctthkqppanyxnnantdoxejanih of orazukdt\n",
      "afujxaannaamutsuaninpzrfkae le jl ansrmvdnanm zhwmbrwahzfqiemnanwqbhsxcobhcdbpnbdjwaxvfqbfannsrcblbn\n",
      "================================================================================\n",
      "Validation set perplexity: 1995.57\n",
      "Average loss at step 100: 5.762220 learning rate: 10.000000\n",
      "Minibatch perplexity: 127.39\n",
      "Validation set perplexity: 115.49\n",
      "Average loss at step 200: 4.595276 learning rate: 10.000000\n",
      "Minibatch perplexity: 83.99\n",
      "Validation set perplexity: 78.29\n",
      "Average loss at step 300: 4.348951 learning rate: 10.000000\n",
      "Minibatch perplexity: 61.54\n",
      "Validation set perplexity: 65.62\n",
      "Average loss at step 400: 4.183363 learning rate: 10.000000\n",
      "Minibatch perplexity: 89.12\n",
      "Validation set perplexity: 56.75\n",
      "Average loss at step 500: 4.157861 learning rate: 9.000000\n",
      "Minibatch perplexity: 66.72\n",
      "Validation set perplexity: 51.91\n",
      "Average loss at step 600: 3.977302 learning rate: 9.000000\n",
      "Minibatch perplexity: 51.16\n",
      "Validation set perplexity: 44.60\n",
      "Average loss at step 700: 3.968611 learning rate: 9.000000\n",
      "Minibatch perplexity: 54.17\n",
      "Validation set perplexity: 43.61\n",
      "Average loss at step 800: 3.902506 learning rate: 9.000000\n",
      "Minibatch perplexity: 52.95\n",
      "Validation set perplexity: 39.23\n",
      "Average loss at step 900: 3.810305 learning rate: 9.000000\n",
      "Minibatch perplexity: 42.12\n",
      "Validation set perplexity: 38.27\n",
      "Average loss at step 1000: 3.771262 learning rate: 8.099999\n",
      "Minibatch perplexity: 48.03\n",
      "================================================================================\n",
      "lnoveferency and the chavextay one nine eight six five end of the ylon the bmiddrs of sing repould d\n",
      "ame two one nine eight four four gifatined agamel theming bass the mates of two rn of d one a epbea \n",
      "tdimo mass the it worler one nine nine eight four the who at the five seven is also one zero four xm\n",
      "docuccvid epn smkimost the nine eight to the new poirgy the number of in one nine eight and four fab\n",
      "bw ands evericon for yeam  kura chriand also mimese dyble for of slam its one nine five up onezoinic\n",
      "================================================================================\n",
      "Validation set perplexity: 36.75\n",
      "Average loss at step 1100: 3.797359 learning rate: 8.099999\n",
      "Minibatch perplexity: 35.17\n",
      "Validation set perplexity: 34.43\n",
      "Average loss at step 1200: 3.780088 learning rate: 8.099999\n",
      "Minibatch perplexity: 38.02\n",
      "Validation set perplexity: 31.50\n",
      "Average loss at step 1300: 3.828670 learning rate: 8.099999\n",
      "Minibatch perplexity: 56.30\n",
      "Validation set perplexity: 29.90\n",
      "Average loss at step 1400: 3.795638 learning rate: 8.099999\n",
      "Minibatch perplexity: 42.96\n",
      "Validation set perplexity: 28.51\n",
      "Average loss at step 1500: 3.774928 learning rate: 7.290000\n",
      "Minibatch perplexity: 43.12\n",
      "Validation set perplexity: 28.35\n",
      "Average loss at step 1600: 3.755915 learning rate: 7.290000\n",
      "Minibatch perplexity: 34.61\n",
      "Validation set perplexity: 26.24\n",
      "Average loss at step 1700: 3.734781 learning rate: 7.290000\n",
      "Minibatch perplexity: 34.27\n",
      "Validation set perplexity: 27.07\n",
      "Average loss at step 1800: 3.736412 learning rate: 7.290000\n",
      "Minibatch perplexity: 34.09\n",
      "Validation set perplexity: 27.95\n",
      "Average loss at step 1900: 3.704492 learning rate: 7.290000\n",
      "Minibatch perplexity: 39.71\n",
      "Validation set perplexity: 27.43\n",
      "Average loss at step 2000: 3.681514 learning rate: 6.560999\n",
      "Minibatch perplexity: 36.45\n",
      "================================================================================\n",
      " xis clavally one seven for in laters the creet movius a s of proverkachs of the mory ade leaddy pro\n",
      "rs maturves durition a publied by was cove natic in booking and the execiaturn womms sfutumes femed \n",
      "yx relevelwran comsicuallyopions a world gover was the lawi by outed in poss of inhystush be monted \n",
      "qkpsident of cold elf six epubling awyal pubmerican geelecto only budgtechanizy au ssg aot suld my m\n",
      "by debod which minogration seved of he rermarihzl repequen may prations the tes name ran in margitf \n",
      "================================================================================\n",
      "Validation set perplexity: 26.89\n",
      "Average loss at step 2100: 3.664712 learning rate: 6.560999\n",
      "Minibatch perplexity: 43.18\n",
      "Validation set perplexity: 25.50\n",
      "Average loss at step 2200: 3.635253 learning rate: 6.560999\n",
      "Minibatch perplexity: 50.15\n",
      "Validation set perplexity: 25.26\n",
      "Average loss at step 2300: 3.583004 learning rate: 6.560999\n",
      "Minibatch perplexity: 29.44\n",
      "Validation set perplexity: 24.93\n",
      "Average loss at step 2400: 3.660956 learning rate: 6.560999\n",
      "Minibatch perplexity: 34.53\n",
      "Validation set perplexity: 25.03\n",
      "Average loss at step 2500: 3.633831 learning rate: 5.904899\n",
      "Minibatch perplexity: 29.10\n",
      "Validation set perplexity: 24.59\n",
      "Average loss at step 2600: 3.637109 learning rate: 5.904899\n",
      "Minibatch perplexity: 42.06\n",
      "Validation set perplexity: 24.20\n",
      "Average loss at step 2700: 3.594821 learning rate: 5.904899\n",
      "Minibatch perplexity: 38.89\n",
      "Validation set perplexity: 23.89\n",
      "Average loss at step 2800: 3.537965 learning rate: 5.904899\n",
      "Minibatch perplexity: 45.17\n",
      "Validation set perplexity: 24.02\n",
      "Average loss at step 2900: 3.574992 learning rate: 5.904899\n",
      "Minibatch perplexity: 43.29\n",
      "Validation set perplexity: 24.40\n",
      "Average loss at step 3000: 3.534893 learning rate: 5.314409\n",
      "Minibatch perplexity: 28.89\n",
      "================================================================================\n",
      "ked to be though eight eight four four one two bap six eight eight zero zero stree at soutku a pange\n",
      "ball for angry two the porditely fature they a lefling including and trists this like so the and coo\n",
      "creane nine nine eight five nine one nine nine five nd the worad of relation of which famostinued da\n",
      "gy as bove the s six sho hintempacdwars ps numh cator that the language elisrate five publisces thre\n",
      "zzjon her eleciatuall he cithoritish mecdhay falkeney the person of resout the am a stassion of dura\n",
      "================================================================================\n",
      "Validation set perplexity: 23.69\n",
      "Average loss at step 3100: 3.516126 learning rate: 5.314409\n",
      "Minibatch perplexity: 30.25\n",
      "Validation set perplexity: 23.44\n",
      "Average loss at step 3200: 3.565601 learning rate: 5.314409\n",
      "Minibatch perplexity: 33.26\n",
      "Validation set perplexity: 23.67\n",
      "Average loss at step 3300: 3.584720 learning rate: 5.314409\n",
      "Minibatch perplexity: 40.26\n",
      "Validation set perplexity: 23.58\n",
      "Average loss at step 3400: 3.561350 learning rate: 5.314409\n",
      "Minibatch perplexity: 29.55\n",
      "Validation set perplexity: 23.79\n",
      "Average loss at step 3500: 3.502506 learning rate: 4.782968\n",
      "Minibatch perplexity: 32.55\n",
      "Validation set perplexity: 22.86\n",
      "Average loss at step 3600: 3.479393 learning rate: 4.782968\n",
      "Minibatch perplexity: 26.44\n",
      "Validation set perplexity: 23.11\n",
      "Average loss at step 3700: 3.459022 learning rate: 4.782968\n",
      "Minibatch perplexity: 29.75\n",
      "Validation set perplexity: 22.77\n",
      "Average loss at step 3800: 3.422221 learning rate: 4.782968\n",
      "Minibatch perplexity: 32.75\n",
      "Validation set perplexity: 22.11\n",
      "Average loss at step 3900: 3.443368 learning rate: 4.782968\n",
      "Minibatch perplexity: 34.42\n",
      "Validation set perplexity: 22.87\n",
      "Average loss at step 4000: 3.530154 learning rate: 4.304671\n",
      "Minibatch perplexity: 38.19\n",
      "================================================================================\n",
      "ibut to no ben sacielder opeblem the zxzking b one four seven one zero five used same sign in in rep\n",
      "pfing in sonday and on the three nine seven salar avoind one eight five eight three seven is quit ne\n",
      "ytt the carry one inyga instrael home a middble the compdbhuless admy d in the ladme yumber the nots\n",
      "xpain nots s take he sheadcrease leader urosit poldiause in for it durs of result one nine one nine \n",
      "fden amon and whos on the dearcharia in two one nine two four eight four seven hotween be with pare \n",
      "================================================================================\n",
      "Validation set perplexity: 22.73\n"
     ]
    }
   ],
   "source": [
    "bigram_vocabulary_size = vocabulary_size * vocabulary_size\n",
    " \n",
    " \n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size_in_chars = len(text)\n",
    "        self._text_size = self._text_size_in_chars // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    " \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            if self._text_size_in_chars - 1 == char_idx:\n",
    "                ch2 = 0\n",
    "            else:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    " \n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    " \n",
    " \n",
    "def bi2str(encoding):\n",
    "    return id2char(encoding // vocabulary_size) + id2char(encoding % vocabulary_size)\n",
    " \n",
    " \n",
    "def bigrams(encodings):\n",
    "    return [bi2str(e) for e in encodings]\n",
    " \n",
    " \n",
    "def bibatches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "    return s\n",
    " \n",
    " \n",
    "bi_onehot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "np.fill_diagonal(bi_onehot, 1)\n",
    " \n",
    " \n",
    "def bigramonehot(encodings):\n",
    "    return [bi_onehot[e] for e in encodings]\n",
    " \n",
    " \n",
    "train_batches = BigramBatchGenerator(train_text, 8, 8)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    " \n",
    "print (bibatches2string(train_batches.next()))\n",
    "print (bibatches2string(train_batches.next()))\n",
    "print (bibatches2string(valid_batches.next()))\n",
    "print (bibatches2string(valid_batches.next()))\n",
    "\n",
    "\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction, size=vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def one_hot_voc(prediction, size=vocabulary_size):\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, prediction[0]] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution(size=vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, size])\n",
    "    return b / np.sum(b, 1)[:, None]\n",
    "\n",
    "\n",
    "\n",
    "num_nodes = 512\n",
    "num_unrollings = 10\n",
    "batch_size = 32\n",
    "embedding_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to all gates\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1), name='x')\n",
    "    # memory of all gates\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name='m')\n",
    "    # biases all gates\n",
    "    biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    # embeddings for all possible bigrams\n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # one hot encoding for labels in\n",
    "    np_one_hot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "    np.fill_diagonal(np_one_hot, 1)\n",
    "    bigram_one_hot = tf.constant(np.reshape(np_one_hot, -1), dtype=tf.float32,\n",
    "                                 shape=[bigram_vocabulary_size, bigram_vocabulary_size])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        mult = tf.matmul(i, x) + tf.matmul(o, m) + biases\n",
    "        input_gate = tf.sigmoid(mult[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(mult[:, num_nodes:num_nodes * 2])\n",
    "        update = mult[:, num_nodes * 3:num_nodes * 4]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(mult[:, num_nodes * 3:])\n",
    "        output = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "    # Input data. [num_unrollings, batch_size] -> one hot encoding removed, we send just bigram ids\n",
    "    tf_train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n",
    "    train_data = list()\n",
    "    for i in tf.split(tf_train_data, num_or_size_splits=num_unrollings + 1, axis=0):\n",
    "        train_data.append(tf.squeeze(i))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for l in train_data[1:]:\n",
    "        train_labels.append(tf.gather(bigram_one_hot, l))\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    # python loop used: tensorflow does not support sequential operations yet\n",
    "    for i in train_inputs:  # having a loop simulates having time\n",
    "        # embed input bigrams -> [batch_size, embedding_size]\n",
    "        output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings, control_dependencies makes sure that output and state are computed\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "        \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 500, 0.9, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # here we predict the embedding\n",
    "    # train_prediction = tf.argmax(tf.nn.softmax(logits), 1, name='train_prediction')\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_output, sample_state = lstm_cell(embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "# initalize batch generators\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        _, l, lr, predictions = session.run([optimizer, loss, learning_rate, train_prediction],\n",
    "                                            feed_dict={tf_train_data: batches, keep_prob: 0.6})\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = list(batches)[1:]\n",
    "            labels = np.concatenate([bi_onehot[l] for l in labels])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = np.argmax(sample(random_distribution(bigram_vocabulary_size), bigram_vocabulary_size))\n",
    "                    sentence = bi2str(feed)\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(49):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed], keep_prob: 1.0})\n",
    "                        feed = np.argmax(sample(prediction, bigram_vocabulary_size))\n",
    "                        sentence += bi2str(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "                # print(predictions)\n",
    "                valid_logprob = valid_logprob + logprob(predictions, one_hot_voc(b[1], bigram_vocabulary_size))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named data_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6ce04c3b53af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_unrollings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anjoy/Desktop/DeepLeanring/seq2seq_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named data_utils"
     ]
    }
   ],
   "source": [
    "import seq2seq_model\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 19\n",
    "\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // num_unrollings\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch(0)\n",
    "\n",
    "    def _next_batch(self, step):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = ''\n",
    "        # print('text size', self._text_size)\n",
    "        for b in range(self._num_unrollings):\n",
    "            # print(self._cursor[step])\n",
    "            self._cursor[step] %= self._text_size\n",
    "            batch += self._text[self._cursor[step]]\n",
    "            self._cursor[step] += 1\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._batch_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def ids(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, ids(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "\n",
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward = []\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1] + ' '\n",
    "    return list(map(lambda x: char2id(x), backward[:-1]))\n",
    "\n",
    "\n",
    "batches = train_batches.next()\n",
    "train_sets = []\n",
    "batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "print('x=', ''.join([id2char(x) for x in batch_encs[0]]))\n",
    "print('y=', ''.join([id2char(x) for x in batch_decs[0]]))\n",
    "\n",
    "\n",
    "def create_model(forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                       target_vocab_size=vocabulary_size,\n",
    "                                       buckets=[(20, 20)],\n",
    "                                       size=256,\n",
    "                                       num_layers=4,\n",
    "                                       max_gradient_norm=5.0,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=1.0,\n",
    "                                       learning_rate_decay_factor=0.9,\n",
    "                                       use_lstm=True,\n",
    "                                       forward_only=forward_only)\n",
    "    return model\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = create_model(False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_steps = 30001\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 100\n",
    "    valid_ckpt = 500\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "        model.batch_size = batch_size\n",
    "        batches = train_batches.next()\n",
    "        train_sets = []\n",
    "        batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "        batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "        for i in range(len(batch_encs)):\n",
    "            train_sets.append((batch_encs[i], batch_decs[i]))\n",
    "\n",
    "        # Get a batch and make a step.\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "        _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "\n",
    "        loss += step_loss / step_ckpt\n",
    "\n",
    "        # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "        if step % step_ckpt == 0:\n",
    "            # Print statistics for the previous epoch.\n",
    "            perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "            print(\"global step %d learning rate %.4f perplexity \"\n",
    "                  \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                sess.run(model.learning_rate_decay_op)\n",
    "            previous_losses.append(loss)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            if step % valid_ckpt == 0:\n",
    "                v_loss = 0.0\n",
    "\n",
    "                model.batch_size = 1\n",
    "                batches = ['the quick brown fox']\n",
    "                test_sets = []\n",
    "                batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "                # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "                test_sets.append((batch_encs[0], []))\n",
    "                # Get a 1-element batch to feed the sentence to the model.\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "                # Get output logits for the sentence.\n",
    "                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "\n",
    "                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "\n",
    "                print('>>>>>>>>> ', batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))\n",
    "\n",
    "                for _ in range(valid_size):\n",
    "                    model.batch_size = 1\n",
    "                    v_batches = valid_batches.next()\n",
    "                    valid_sets = []\n",
    "                    v_batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), v_batches))\n",
    "                    v_batch_decs = list(map(lambda x: rev_id(x), v_batches))\n",
    "                    for i in range(len(v_batch_encs)):\n",
    "                        valid_sets.append((v_batch_encs[i], v_batch_decs[i]))\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                    v_loss += eval_loss / valid_size\n",
    "\n",
    "                eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "                print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "\n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "    # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0], []))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "    print(batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
